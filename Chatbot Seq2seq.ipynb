{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/Ai Notebooks/Chatbot-data/clean_conversation.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(600, len(lines) - 1)]:\n",
    "    input_t = line.split('\\t')[0]\n",
    "    target_t = line.split('\\t')[1]\n",
    "    input_text.append(input_t)\n",
    "    target_text.append(target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_list = list(zip(input_text, target_text))\n",
    "lines = pd.DataFrame(zipped_list, columns=['input', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are your interests</td>\n",
       "      <td>I am interested in all kinds of things. We can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are your favorite subjects</td>\n",
       "      <td>My favorite subjects include robotics, compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are your interests</td>\n",
       "      <td>I am interested in a wide variety of topics, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is your number</td>\n",
       "      <td>I don't have any number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is your number</td>\n",
       "      <td>23 skiddoo!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             input  \\\n",
       "0          What are your interests   \n",
       "1  What are your favorite subjects   \n",
       "2          What are your interests   \n",
       "3              What is your number   \n",
       "4              What is your number   \n",
       "\n",
       "                                              output  \n",
       "0  I am interested in all kinds of things. We can...  \n",
       "1  My favorite subjects include robotics, compute...  \n",
       "2  I am interested in a wide variety of topics, a...  \n",
       "3                            I don't have any number  \n",
       "4                                        23 skiddoo!  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data for Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input max length is 22\n",
      "Encoder input data shape -> (566, 22)\n",
      "Number of Input tokens = 518\n"
     ]
    }
   ],
   "source": [
    "input_lines = list()\n",
    "for line in lines.input:\n",
    "    input_lines.append(line)\n",
    "    \n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(input_lines)\n",
    "tokenized_input_lines = tokenizer.texts_to_sequences(input_lines)\n",
    "\n",
    "len_list = list()\n",
    "for token_line in tokenized_input_lines:\n",
    "    len_list.append(len(token_line))\n",
    "max_len = np.array(len_list).max()\n",
    "print( 'Input max length is {}'.format( max_len ))\n",
    "\n",
    "padded_input_lines = preprocessing.sequence.pad_sequences(tokenized_input_lines, maxlen=max_len, padding='post')\n",
    "encoder_input_data = np.array(padded_input_lines)\n",
    "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
    "\n",
    "input_word_dict = tokenizer.word_index\n",
    "num_input_tokens = len(input_word_dict) + 1 \n",
    "print( 'Number of Input tokens = {}'.format( num_input_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input for Decoder: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output max length is 74\n",
      "Decoder input data shape -> (566, 74)\n",
      "Number of Input tokens = 1692\n"
     ]
    }
   ],
   "source": [
    "output_lines = list()\n",
    "for line in lines.output:\n",
    "    output_lines.append('<START> ' + line +  ' <END>')\n",
    "    \n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(output_lines)\n",
    "tokenized_output_lines = tokenizer.texts_to_sequences(output_lines)\n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_output_lines:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_output_length = np.array( length_list ).max()\n",
    "print( 'Output max length is {}'.format( max_output_length ))\n",
    "\n",
    "padded_output_lines = preprocessing.sequence.pad_sequences(tokenized_output_lines, maxlen=max_output_length, padding='post')\n",
    "decoder_input_data = np.array(padded_output_lines)\n",
    "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
    "\n",
    "output_word_dict = tokenizer.word_index\n",
    "num_output_tokens = len(output_word_dict) + 1 \n",
    "print( 'Number of Input tokens = {}'.format( num_output_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare target data for decoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder target data shape -> (566, 74, 1692)\n"
     ]
    }
   ],
   "source": [
    "decoder_target_data = list()\n",
    "for token in tokenized_output_lines:\n",
    "    decoder_target_data.append(token[1:])\n",
    "    \n",
    "padded_output_lines = preprocessing.sequence.pad_sequences(decoder_target_data, maxlen=max_output_length, padding='post')\n",
    "onehot_output_lines = utils.to_categorical(padded_output_lines, num_output_tokens)\n",
    "decoder_target_data = np.array(onehot_output_lines)\n",
    "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dream Walker\\anaconda3\\envs\\Ai\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    132608      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    433152      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 525312      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  525312      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1692)   434844      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,051,228\n",
      "Trainable params: 2,051,228\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_input_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_output_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_output_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 1.4406\n",
      "Epoch 2/250\n",
      "566/566 [==============================] - 10s 18ms/sample - loss: 1.4348\n",
      "Epoch 3/250\n",
      "566/566 [==============================] - 11s 20ms/sample - loss: 1.3829\n",
      "Epoch 4/250\n",
      "566/566 [==============================] - 11s 20ms/sample - loss: 1.2243\n",
      "Epoch 5/250\n",
      "566/566 [==============================] - 13s 23ms/sample - loss: 1.1490\n",
      "Epoch 6/250\n",
      "566/566 [==============================] - 14s 24ms/sample - loss: 1.1212\n",
      "Epoch 7/250\n",
      "566/566 [==============================] - 15s 27ms/sample - loss: 1.1100\n",
      "Epoch 8/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 1.1035\n",
      "Epoch 9/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 1.0962\n",
      "Epoch 10/250\n",
      "566/566 [==============================] - 16s 27ms/sample - loss: 1.0905\n",
      "Epoch 11/250\n",
      "566/566 [==============================] - 14s 25ms/sample - loss: 1.0832\n",
      "Epoch 12/250\n",
      "566/566 [==============================] - 13s 23ms/sample - loss: 1.0764\n",
      "Epoch 13/250\n",
      "566/566 [==============================] - 13s 24ms/sample - loss: 1.0680\n",
      "Epoch 14/250\n",
      "566/566 [==============================] - 14s 24ms/sample - loss: 1.0601\n",
      "Epoch 15/250\n",
      "566/566 [==============================] - 13s 24ms/sample - loss: 1.0518\n",
      "Epoch 16/250\n",
      "566/566 [==============================] - 15s 27ms/sample - loss: 1.0439\n",
      "Epoch 17/250\n",
      "566/566 [==============================] - 15s 27ms/sample - loss: 1.0364\n",
      "Epoch 18/250\n",
      "566/566 [==============================] - 14s 24ms/sample - loss: 1.0295\n",
      "Epoch 19/250\n",
      "566/566 [==============================] - 13s 23ms/sample - loss: 1.0232\n",
      "Epoch 20/250\n",
      "566/566 [==============================] - 14s 25ms/sample - loss: 1.0168\n",
      "Epoch 21/250\n",
      "566/566 [==============================] - 15s 27ms/sample - loss: 1.0104\n",
      "Epoch 22/250\n",
      "566/566 [==============================] - 21s 38ms/sample - loss: 1.0041\n",
      "Epoch 23/250\n",
      "566/566 [==============================] - 18s 32ms/sample - loss: 0.9965\n",
      "Epoch 24/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.9890\n",
      "Epoch 25/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.9815\n",
      "Epoch 26/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.9736\n",
      "Epoch 27/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.9658\n",
      "Epoch 28/250\n",
      "566/566 [==============================] - 14s 25ms/sample - loss: 0.9579\n",
      "Epoch 29/250\n",
      "566/566 [==============================] - 16s 28ms/sample - loss: 0.9502\n",
      "Epoch 30/250\n",
      "566/566 [==============================] - 16s 28ms/sample - loss: 0.9422\n",
      "Epoch 31/250\n",
      "566/566 [==============================] - 16s 28ms/sample - loss: 0.9345\n",
      "Epoch 32/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.9265\n",
      "Epoch 33/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.9188\n",
      "Epoch 34/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.9110\n",
      "Epoch 35/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.9036\n",
      "Epoch 36/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.8965\n",
      "Epoch 37/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.8886\n",
      "Epoch 38/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.8813\n",
      "Epoch 39/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.8736\n",
      "Epoch 40/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.8666\n",
      "Epoch 41/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.8589\n",
      "Epoch 42/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.8514\n",
      "Epoch 43/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.8440\n",
      "Epoch 44/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.8371\n",
      "Epoch 45/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.8296\n",
      "Epoch 46/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.8226\n",
      "Epoch 47/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.8158\n",
      "Epoch 48/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.8087\n",
      "Epoch 49/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.8021\n",
      "Epoch 50/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.7950\n",
      "Epoch 51/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.7887\n",
      "Epoch 52/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.7824\n",
      "Epoch 53/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.7762\n",
      "Epoch 54/250\n",
      "566/566 [==============================] - 18s 32ms/sample - loss: 0.7699\n",
      "Epoch 55/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.7636\n",
      "Epoch 56/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.7579\n",
      "Epoch 57/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.7516\n",
      "Epoch 58/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.7450\n",
      "Epoch 59/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.7392\n",
      "Epoch 60/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.7329\n",
      "Epoch 61/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.7271\n",
      "Epoch 62/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.7204\n",
      "Epoch 63/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.7148\n",
      "Epoch 64/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.7090\n",
      "Epoch 65/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.7032\n",
      "Epoch 66/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.6972\n",
      "Epoch 67/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.6914\n",
      "Epoch 68/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.6854\n",
      "Epoch 69/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.6794\n",
      "Epoch 70/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.6737\n",
      "Epoch 71/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.6685\n",
      "Epoch 72/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.6622\n",
      "Epoch 73/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.6565\n",
      "Epoch 74/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.6507\n",
      "Epoch 75/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.6451\n",
      "Epoch 76/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.6392\n",
      "Epoch 77/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.6339\n",
      "Epoch 78/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.6273\n",
      "Epoch 79/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.6217\n",
      "Epoch 80/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.6158\n",
      "Epoch 81/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.6106\n",
      "Epoch 82/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.6051\n",
      "Epoch 83/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.5987\n",
      "Epoch 84/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.5931\n",
      "Epoch 85/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.5876\n",
      "Epoch 86/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.5820\n",
      "Epoch 87/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.5762\n",
      "Epoch 88/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.5698\n",
      "Epoch 89/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.5636\n",
      "Epoch 90/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.5575\n",
      "Epoch 91/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.5513\n",
      "Epoch 92/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.5459\n",
      "Epoch 93/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.5406\n",
      "Epoch 94/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.5347\n",
      "Epoch 95/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.5289\n",
      "Epoch 96/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.5232\n",
      "Epoch 97/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.5183\n",
      "Epoch 98/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.5125\n",
      "Epoch 99/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.5081\n",
      "Epoch 100/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.5021\n",
      "Epoch 101/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.4964\n",
      "Epoch 102/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.4910\n",
      "Epoch 103/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4861\n",
      "Epoch 104/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4804\n",
      "Epoch 105/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4756\n",
      "Epoch 106/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.4704\n",
      "Epoch 107/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.4649\n",
      "Epoch 108/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.4595\n",
      "Epoch 109/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.4553\n",
      "Epoch 110/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4498\n",
      "Epoch 111/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.4449\n",
      "Epoch 112/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4391\n",
      "Epoch 113/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4340\n",
      "Epoch 114/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4300\n",
      "Epoch 115/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.4247\n",
      "Epoch 116/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.4193\n",
      "Epoch 117/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4154\n",
      "Epoch 118/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4112\n",
      "Epoch 119/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4052\n",
      "Epoch 120/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.4018\n",
      "Epoch 121/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3964\n",
      "Epoch 122/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.3922\n",
      "Epoch 123/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3871\n",
      "Epoch 124/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3835\n",
      "Epoch 125/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.3783\n",
      "Epoch 126/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.3744\n",
      "Epoch 127/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.3696\n",
      "Epoch 128/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3650\n",
      "Epoch 129/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.3616\n",
      "Epoch 130/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3571\n",
      "Epoch 131/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3530\n",
      "Epoch 132/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.3490\n",
      "Epoch 133/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.3443\n",
      "Epoch 134/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.3411\n",
      "Epoch 135/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.3371\n",
      "Epoch 136/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3330\n",
      "Epoch 137/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3290\n",
      "Epoch 138/250\n",
      "566/566 [==============================] - 16s 29ms/sample - loss: 0.3256\n",
      "Epoch 139/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.3214\n",
      "Epoch 140/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.3165\n",
      "Epoch 141/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.3137\n",
      "Epoch 142/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.3105\n",
      "Epoch 143/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3060\n",
      "Epoch 144/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3033\n",
      "Epoch 145/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.3003\n",
      "Epoch 146/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2959\n",
      "Epoch 147/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2931\n",
      "Epoch 148/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.2891\n",
      "Epoch 149/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2861\n",
      "Epoch 150/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.2819\n",
      "Epoch 151/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.2789\n",
      "Epoch 152/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2766\n",
      "Epoch 153/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2727\n",
      "Epoch 154/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2691\n",
      "Epoch 155/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2654\n",
      "Epoch 156/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2639\n",
      "Epoch 157/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2602\n",
      "Epoch 158/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2571\n",
      "Epoch 159/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2544\n",
      "Epoch 160/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2513\n",
      "Epoch 161/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.2483\n",
      "Epoch 162/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.2458\n",
      "Epoch 163/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2436\n",
      "Epoch 164/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2401\n",
      "Epoch 165/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2368\n",
      "Epoch 166/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2349\n",
      "Epoch 167/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2313\n",
      "Epoch 168/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2299\n",
      "Epoch 169/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.2271\n",
      "Epoch 170/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2233\n",
      "Epoch 171/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2221\n",
      "Epoch 172/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.2195\n",
      "Epoch 173/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.2167\n",
      "Epoch 174/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.2143\n",
      "Epoch 175/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.2112\n",
      "Epoch 176/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.2101\n",
      "Epoch 177/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2071\n",
      "Epoch 178/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2045\n",
      "Epoch 179/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.2024\n",
      "Epoch 180/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.2011\n",
      "Epoch 181/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1974\n",
      "Epoch 182/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1954\n",
      "Epoch 183/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1936\n",
      "Epoch 184/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1916\n",
      "Epoch 185/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1891\n",
      "Epoch 186/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1864\n",
      "Epoch 187/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1853\n",
      "Epoch 188/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1840\n",
      "Epoch 189/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1815\n",
      "Epoch 190/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1788\n",
      "Epoch 191/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1774\n",
      "Epoch 192/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1750\n",
      "Epoch 193/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1735\n",
      "Epoch 194/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1721\n",
      "Epoch 195/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1703\n",
      "Epoch 196/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1676\n",
      "Epoch 197/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1664\n",
      "Epoch 198/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1641\n",
      "Epoch 199/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1630\n",
      "Epoch 200/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1610\n",
      "Epoch 201/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1591\n",
      "Epoch 202/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1577\n",
      "Epoch 203/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1549\n",
      "Epoch 204/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1531\n",
      "Epoch 205/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1527\n",
      "Epoch 206/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1501\n",
      "Epoch 207/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1486\n",
      "Epoch 208/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1476\n",
      "Epoch 209/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1454\n",
      "Epoch 210/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1455\n",
      "Epoch 211/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1433\n",
      "Epoch 212/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1418\n",
      "Epoch 213/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1401\n",
      "Epoch 214/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1380\n",
      "Epoch 215/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1371\n",
      "Epoch 216/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1351\n",
      "Epoch 217/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1354\n",
      "Epoch 218/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1327\n",
      "Epoch 219/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1307\n",
      "Epoch 220/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1304\n",
      "Epoch 221/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1290\n",
      "Epoch 222/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1280\n",
      "Epoch 223/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1264\n",
      "Epoch 224/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1252\n",
      "Epoch 225/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1240\n",
      "Epoch 226/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1220\n",
      "Epoch 227/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1210\n",
      "Epoch 228/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1202\n",
      "Epoch 229/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1186\n",
      "Epoch 230/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1184\n",
      "Epoch 231/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1162\n",
      "Epoch 232/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1147\n",
      "Epoch 233/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1146\n",
      "Epoch 234/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1128\n",
      "Epoch 235/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1121\n",
      "Epoch 236/250\n",
      "566/566 [==============================] - 17s 29ms/sample - loss: 0.1108\n",
      "Epoch 237/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1100\n",
      "Epoch 238/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1091\n",
      "Epoch 239/250\n",
      "566/566 [==============================] - 18s 31ms/sample - loss: 0.1077\n",
      "Epoch 240/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1065\n",
      "Epoch 241/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1057\n",
      "Epoch 242/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1051\n",
      "Epoch 243/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1029\n",
      "Epoch 244/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.1028\n",
      "Epoch 245/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.1020\n",
      "Epoch 246/250\n",
      "566/566 [==============================] - 17s 31ms/sample - loss: 0.0999\n",
      "Epoch 247/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.0997\n",
      "Epoch 248/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.0982\n",
      "Epoch 249/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.0982\n",
      "Epoch 250/250\n",
      "566/566 [==============================] - 17s 30ms/sample - loss: 0.0971\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=124, epochs=250) \n",
    "model.save( 'model.h5' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_model():\n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(256,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(256,))\n",
    "    \n",
    "    decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, decoder_state_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_token (sentence: str):\n",
    "    words = sentence.lower().split()\n",
    "    token_list = list()\n",
    "    for word in words: \n",
    "        token_list.append(input_word_dict[word])\n",
    "    return preprocessing.sequence.pad_sequences([token_list], maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi\n",
      "Bot: hello\n",
      "\n",
      "User: how are you\n",
      "Bot: i certainly am i shouldn't try so hard\n",
      "\n",
      "User: what is ai\n",
      "Bot: ai is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_model()\n",
    "\n",
    "enc_model.save( 'enc_model.h5' ) \n",
    "dec_model.save( 'dec_model.h5' ) \n",
    "model.save( 'model.h5' ) \n",
    "\n",
    "for epoch in range( encoder_input_data.shape[0] ):\n",
    "    states_values = enc_model.predict( str_to_token( input( 'User: ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = output_word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in output_word_dict.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( \"Bot:\" +decoded_translation.replace(' end', '') )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
